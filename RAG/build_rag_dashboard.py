#!/usr/bin/env python3
"""Aggregate RAG summary artifacts and render a Plotly HTML dashboard.

The script expects the ``RAG`` directory to contain per-run folders (for example
``llama3.2-3b-ragres1``). Each run folder must include a model directory that
stores ``summary.json`` and ``summary_metrics.json`` generated by
``rag_benchmark.py``. Metrics pulled into the dashboard:

* Average retrieval latency (``summary.json``)
* Average prompt-eval latency (``summary_metrics.json``)
* Semantic similarity average (``summary.json``)
* Aggregate F1 score (``summary.json``)
* Session-level carbon emissions (``summary.json``)

The resulting HTML is self-contained (loads Plotly from CDN) and highlights
accuracy, latency, and carbon trade-offs across models.
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional

import plotly.graph_objects as go
import plotly.io as pio


@dataclass
class ModelReport:
    label: str
    retrieval_ms: Optional[float]
    load_ms: Optional[float]
    prompt_eval_ms: Optional[float]
    eval_ms: Optional[float]
    semantic_similarity: Optional[float]
    f1: Optional[float]
    carbon_kg: Optional[float]
    question_count: Optional[int]
    summary_path: Path
    summary_metrics_path: Path

    @property
    def total_latency_ms(self) -> Optional[float]:
        generation = self.generation_latency_ms
        if self.retrieval_ms is None or generation is None:
            return None
        return self.retrieval_ms + generation

    @property
    def blended_accuracy(self) -> Optional[float]:
        if self.f1 is None or self.semantic_similarity is None:
            return None
        return 0.5 * (self.f1 + self.semantic_similarity)

    @property
    def generation_latency_ms(self) -> Optional[float]:
        components = [self.load_ms, self.prompt_eval_ms, self.eval_ms]
        if any(component is None for component in components):
            return None
        return sum(component for component in components if component is not None)

    @property
    def per_question_carbon(self) -> Optional[float]:
        if self.carbon_kg is None or not self.question_count:
            return None
        return self.carbon_kg / float(self.question_count)

    def as_table_row(self) -> Dict[str, Optional[float]]:
        return {
            "Model": self.label,
            "Retrieval (ms)": self.retrieval_ms,
            "Load (ms)": self.load_ms,
            "Prompt Eval (ms)": self.prompt_eval_ms,
            "Eval (ms)": self.eval_ms,
            "Generation (ms)": self.generation_latency_ms,
            "Total Latency (ms)": self.total_latency_ms,
            "Semantic Similarity": self.semantic_similarity,
            "F1": self.f1,
            "Avg Accuracy": self.blended_accuracy,
            "Carbon (kg)": self.carbon_kg,
            "Carbon / Question (kg)": self.per_question_carbon,
        }


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build a RAG comparison dashboard")
    parser.add_argument(
        "--rag-root",
        default=".",
        help="Path containing per-model RAG results (default: current directory)",
    )
    parser.add_argument(
        "--output",
        default="rag_dashboard.html",
        help="Path to the generated HTML dashboard",
    )
    return parser.parse_args()


def _load_json(path: Path) -> Dict[str, object]:
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def collect_reports(root: Path) -> List[ModelReport]:
    reports: List[ModelReport] = []
    for summary_path in sorted(root.glob("*/*/summary.json")):
        summary_metrics_path = summary_path.with_name("summary_metrics.json")
        if not summary_metrics_path.exists():
            continue
        summary_payload = _load_json(summary_path)
        summary_metrics_payload = _load_json(summary_metrics_path)

        retrieval_block = summary_payload.get("retrieval_latency_ms") or {}
        evaluation_block = summary_payload.get("evaluation") or {}
        metrics_root = (summary_metrics_payload.get("metrics") or {})

        report = ModelReport(
            label=summary_path.parent.name,
            retrieval_ms=_safe_float(retrieval_block, "avg"),
            load_ms=_metric_avg(metrics_root, "load_time_ms"),
            prompt_eval_ms=_metric_avg(metrics_root, "prompt_eval_time_ms"),
            eval_ms=_metric_avg(metrics_root, "eval_time_ms"),
            semantic_similarity=_safe_float(summary_payload, "semantic_similarity_avg"),
            f1=_safe_float(evaluation_block, "f1"),
            carbon_kg=_safe_float(summary_payload, "session_carbon_emissions_kg"),
            question_count=_safe_int(summary_payload, "question_count")
            or _safe_int(evaluation_block, "question_count"),
            summary_path=summary_path,
            summary_metrics_path=summary_metrics_path,
        )
        reports.append(report)
    return reports


def _safe_float(container: Dict[str, object], key: str) -> Optional[float]:
    value = container.get(key)
    if isinstance(value, (int, float)):
        return float(value)
    return None


def _safe_int(container: Dict[str, object], key: str) -> Optional[int]:
    value = container.get(key)
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        return int(value)
    return None


def _metric_avg(metrics_root: Dict[str, object], metric_name: str) -> Optional[float]:
    block = metrics_root.get(metric_name)
    if isinstance(block, dict):
        return _safe_float(block, "avg")
    return None


def build_accuracy_chart(reports: List[ModelReport]) -> go.Figure:
    labels = [r.label for r in reports]
    f1_values = [r.f1 for r in reports]
    semantic_values = [r.semantic_similarity for r in reports]

    fig = go.Figure()
    fig.add_bar(name="F1", x=labels, y=f1_values, marker_color="#2563eb")
    fig.add_bar(name="Semantic Similarity", x=labels, y=semantic_values, marker_color="#f97316")
    fig.update_layout(
        title="Accuracy Signals",
        xaxis_title="Model",
        yaxis_title="Score",
        barmode="group",
        template="plotly_white",
    )
    return fig


def build_latency_chart(reports: List[ModelReport]) -> go.Figure:
    labels = [r.label for r in reports]
    retrieval_values = [r.retrieval_ms for r in reports]
    load_values = [r.load_ms for r in reports]
    prompt_values = [r.prompt_eval_ms for r in reports]
    eval_values = [r.eval_ms for r in reports]

    fig = go.Figure()
    fig.add_bar(name="Retrieval", x=labels, y=retrieval_values, marker_color="#0ea5e9")
    fig.add_bar(name="Load", x=labels, y=load_values, marker_color="#a855f7")
    fig.add_bar(name="Prompt Eval", x=labels, y=prompt_values, marker_color="#22c55e")
    fig.add_bar(name="Eval", x=labels, y=eval_values, marker_color="#f97316")
    fig.update_layout(
        title="Latency Breakdown (ms)",
        xaxis_title="Model",
        yaxis_title="Milliseconds",
        barmode="stack",
        template="plotly_white",
    )
    return fig


def build_carbon_chart(reports: List[ModelReport]) -> go.Figure:
    labels = [r.label for r in reports]
    carbon_values = [r.carbon_kg for r in reports]
    per_question_values = [r.per_question_carbon for r in reports]

    fig = go.Figure()
    fig.add_bar(name="Session", x=labels, y=carbon_values, marker_color="#94a3b8")
    fig.add_bar(name="Per Question", x=labels, y=per_question_values, marker_color="#d946ef")
    fig.update_layout(
        title="Carbon Emissions (kg)",
        xaxis_title="Model",
        yaxis_title="Kilograms",
        template="plotly_white",
        barmode="group",
    )
    return fig


def build_table_html(reports: List[ModelReport]) -> str:
    headers = [
        "Model",
        "Retrieval (ms)",
        "Load (ms)",
        "Prompt Eval (ms)",
        "Eval (ms)",
        "Generation (ms)",
        "Total Latency (ms)",
        "Semantic Similarity",
        "F1",
        "Avg Accuracy",
        "Carbon (kg)",
        "Carbon / Question (kg)",
    ]
    rows = [report.as_table_row() for report in reports]

    def _fmt(value) -> str:  # type: ignore[override]
        if value is None:
            return "—"
        if isinstance(value, str):
            return value
        if not isinstance(value, (int, float)):
            return "—"
        if abs(value) >= 1:
            return f"{float(value):,.3f}"
        return f"{float(value):.3e}"

    body_rows = "\n".join(
        "<tr>" + "".join(f"<td>{_fmt(row[h])}</td>" for h in headers) + "</tr>"
        for row in rows
    )
    header_html = "".join(f"<th>{header}</th>" for header in headers)
    return (
        "<table class=\"metrics-table\">"
        f"<thead><tr>{header_html}</tr></thead>"
        f"<tbody>{body_rows}</tbody>"
        "</table>"
    )


def build_insights(reports: List[ModelReport]) -> str:
    bullets: List[str] = []
    best_f1 = _best_by(reports, key=lambda r: r.f1)
    if best_f1:
        bullets.append(f"<li><strong>{best_f1.label}</strong> leads F1 at {best_f1.f1:.3f}.</li>")
    best_semantic = _best_by(reports, key=lambda r: r.semantic_similarity)
    if best_semantic and best_semantic is not best_f1:
        bullets.append(
            f"<li><strong>{best_semantic.label}</strong> has the highest semantic overlap ({best_semantic.semantic_similarity:.3f}).</li>"
        )
    lowest_latency = _best_by(reports, key=lambda r: r.total_latency_ms, reverse=False)
    if lowest_latency:
        bullets.append(
            f"<li><strong>{lowest_latency.label}</strong> is fastest overall ({lowest_latency.total_latency_ms:.1f} ms combined).</li>"
        )
    lowest_carbon = _best_by(reports, key=lambda r: r.carbon_kg, reverse=False)
    if lowest_carbon:
        bullets.append(
            f"<li><strong>{lowest_carbon.label}</strong> has the smallest session emissions ({lowest_carbon.carbon_kg:.3e} kg).</li>"
        )
    if not bullets:
        bullets.append("<li>No insights available (missing metrics).</li>")
    return "<ul>" + "".join(bullets) + "</ul>"


def build_accuracy_vs_carbon_chart(reports: List[ModelReport]) -> go.Figure:
    labels = []
    accuracies = []
    carbons = []
    for report in reports:
        if report.blended_accuracy is None or report.carbon_kg is None:
            continue
        labels.append(report.label)
        accuracies.append(report.blended_accuracy)
        carbons.append(report.carbon_kg)

    fig = go.Figure(
        data=[
            go.Scatter(
                x=carbons,
                y=accuracies,
                mode="markers+text",
                text=labels,
                textposition="top center",
                marker=dict(size=14, color="#dc2626", line=dict(width=1, color="#1e1b4b")),
            )
        ]
    )
    fig.update_layout(
        title="Avg Accuracy vs Carbon",
        xaxis_title="Session Carbon Emissions (kg)",
        yaxis_title="Blended Accuracy ( (F1 + Semantic)/2 )",
        template="plotly_white",
    )
    return fig


def _best_by(
    reports: List[ModelReport],
    key,
    reverse: bool = True,
) -> Optional[ModelReport]:
    scored = [(key(report), report) for report in reports]
    scored = [(value, report) for value, report in scored if isinstance(value, (int, float))]
    if not scored:
        return None
    value, report = max(scored, key=lambda pair: pair[0]) if reverse else min(scored, key=lambda pair: pair[0])
    return report


def render_dashboard(reports: List[ModelReport], output: Path) -> None:
    if not reports:
        raise SystemExit("No summary files found under the provided RAG root")

    accuracy_html = pio.to_html(
        build_accuracy_chart(reports),
        include_plotlyjs="cdn",
        full_html=False,
        div_id="accuracy",
    )
    latency_html = pio.to_html(
        build_latency_chart(reports),
        include_plotlyjs=False,
        full_html=False,
        div_id="latency",
    )
    carbon_html = pio.to_html(
        build_carbon_chart(reports),
        include_plotlyjs=False,
        full_html=False,
        div_id="carbon",
    )
    accuracy_vs_carbon_html = pio.to_html(
        build_accuracy_vs_carbon_chart(reports),
        include_plotlyjs=False,
        full_html=False,
        div_id="accuracy_vs_carbon",
    )

    insights_html = build_insights(reports)

    html = f"""<!DOCTYPE html>
<html lang=\"en\">
<head>
<meta charset=\"utf-8\" />
<title>RAG Benchmark Dashboard</title>
<style>
body {{ font-family: Arial, sans-serif; margin: 0; padding: 2rem; background: #f8fafc; }}
h1 {{ margin-top: 0; }}
section {{ margin-bottom: 2rem; background: #fff; padding: 1.5rem; border-radius: 0.75rem; box-shadow: 0 10px 25px rgba(15, 23, 42, 0.08); }}
.metrics-table {{ width: 100%; border-collapse: collapse; font-size: 0.95rem; }}
.metrics-table th, .metrics-table td {{ text-align: right; padding: 0.5rem 0.75rem; border-bottom: 1px solid #e2e8f0; }}
.metrics-table th:first-child, .metrics-table td:first-child {{ text-align: left; }}
ul {{ margin: 0; padding-left: 1.25rem; }}
</style>
</head>
<body>
<h1>RAG Benchmark Overview</h1>
<section>
<h2>Quick Insights</h2>
{insights_html}
</section>
<section>
<h2>Accuracy</h2>
{accuracy_html}
</section>
<section>
<h2>Latency</h2>
{latency_html}
</section>
<section>
<h2>Carbon Footprint</h2>
{carbon_html}
</section>
<section>
<h2>Accuracy vs Carbon</h2>
{accuracy_vs_carbon_html}
</section>
</body>
</html>
"""
    output.write_text(html, encoding="utf-8")


def main() -> None:
    args = parse_args()
    root = Path(args.rag_root).resolve()
    reports = collect_reports(root)
    render_dashboard(reports, Path(args.output).resolve())


if __name__ == "__main__":
    main()
